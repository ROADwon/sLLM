{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67617df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft trl wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b9e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c6cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --force-reinstall typing-extensions==4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94c83c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec30c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb90021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7060f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "174a12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f55cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash-attn==2.5.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip freeze | grep flash-attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ababb9",
   "metadata": {},
   "source": [
    "## dataset train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1edf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.0' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd58a29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cef7fad",
   "metadata": {},
   "source": [
    "## Notice! \n",
    "-> if you have some problem with datasets library or transformers library\n",
    "---\n",
    "Problem 1. error \"no module named dataset\" <br>\n",
    "solution 1. !pip3 install datasets<br>\n",
    "---\n",
    "Problem 2. huggingface_hub Error <br>\n",
    "Solution 2. ! pip3 install -U transformers <br>\n",
    "\n",
    "--- \n",
    "are those errors belong GPU session was closed,\n",
    "all the installation information was formatting \n",
    "so, if you restart GPU session, you must reinstall all the library, when install library or file before closed session\n",
    "not Kernel restart only GPU session restart\n",
    "---\n",
    "and also follow this solution\n",
    "1. pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d78c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze >> requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da3bae",
   "metadata": {},
   "source": [
    "### before Starting make a simple function \n",
    "#### this function use for working clock to parameter value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a300c7",
   "metadata": {},
   "source": [
    "## CUDA load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef28f4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67db2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype=torch.bfloat16\n",
    "else :\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c000acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flash_attention_2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "264bd2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27263a3",
   "metadata": {},
   "source": [
    "## Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e29e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f64fe54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5523d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "641a9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b-it\"\n",
    "tokenizer_name = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101dba8",
   "metadata": {},
   "source": [
    "## Checking Datasets Type and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef9f957b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'prompt'],\n",
       "    num_rows: 18612\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e4ff487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18612, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4c79d",
   "metadata": {},
   "source": [
    "### using dataset's train_test_split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b21ce116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt'],\n",
       "        num_rows: 13028\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'prompt'],\n",
       "        num_rows: 5584\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b76c48",
   "metadata": {},
   "source": [
    "## using sklearn.model_selection's train_test_split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5205c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "936a4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_sklearn = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4a4c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97205963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, test_set = train_test_split(ds_sklearn[\"train\"], test_size=.3, random_state=1832)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0a210a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2730229",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2011786",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config=BitsAndBytesConfig(load_in_4bit=True,\n",
    "                             bnb_4bit_quant_type=\"nf4\",\n",
    "                             bnb_4bit_compute_dtype=torch_dtype,\n",
    "                             bnb_4bit_use_double_quant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3f69d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(r=16,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=.05,\n",
    "                        bias=\"none\",\n",
    "                        task_type=\"CAUSAL_LM\",\n",
    "                        target_modules=[\"up_proj\",\"donw_proj\",\"gate_proj\",\"k_proj\",\n",
    "                                       \"q_proj\",\"v_proj\",\"o_proj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03381181",
   "metadata": {},
   "source": [
    "## model & tokenizer load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b970db0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: you need flash_attn package version to be greater or equal than 2.1.0. Detected version 0.2.8.dev0. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_implementation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:3544\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   3543\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m-> 3544\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:1454\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1454\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;66;03m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_sdpa(\n\u001b[1;32m   1464\u001b[0m         config,\n\u001b[1;32m   1465\u001b[0m         hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1466\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:1551\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flash_attention_version \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   1552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m you need flash_attn package version to be greater or equal than 2.1.0. Detected version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflash_attention_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1553\u001b[0m         )\n\u001b[1;32m   1554\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Flash Attention 2 is not available. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: you need flash_attn package version to be greater or equal than 2.1.0. Detected version 0.2.8.dev0. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                          token=access_token,\n",
    "                                          device_map=\"auto\",\n",
    "                                            attn_implementation=attn_implementation,\n",
    "                                           quantization_config=bnb_config\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
    "                                         token=access_token,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         max_length=100,\n",
    "                                         )\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b559f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12d9df",
   "metadata": {},
   "source": [
    "## processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e80754",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22820546",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ds[\"test\"]\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e24bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row):\n",
    "    return tokenizer(row[\"instruction\"],row[\"input\"],row[\"output\"],row[\"prompt\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81519ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(process,\n",
    "           num_proc = multiprocessing.cpu_count(),\n",
    "           load_from_cache_file=False,\n",
    "           batched=True)\n",
    "train_dataset = ds[\"train\"]\n",
    "test_dataset = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706135f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b069a",
   "metadata": {},
   "source": [
    "## model & Trainer arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b333044",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./eval_results_4\",\n",
    "                                 num_train_epochs=3,\n",
    "                                 per_device_train_batch_size=2,\n",
    "                                 per_device_eval_batch_size=2,\n",
    "                                 weight_decay=0.01,\n",
    "                                 logging_dir=\"./eval_logs_4\",\n",
    "                                 logging_steps=500,\n",
    "                                 warmup_steps=300,\n",
    "                                 dataloader_num_workers=4,\n",
    "                                 eval_accumulation_steps=1,\n",
    "                                 gradient_accumulation_steps=2,\n",
    "                                 optim=\"adamw_torch\",\n",
    "                                 evaluation_strategy=\"steps\",\n",
    "                                 save_strategy=\"steps\",\n",
    "                                 do_eval=True,\n",
    "                                 load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef106d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c707991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_metrix = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7506f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_matrix(eval_pred):\n",
    "    logit, labels =eval_pred\n",
    "    predict = np.argmax(logit, axis=-1)\n",
    "    return acc_metrix.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf826e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer = Trainer(model,\n",
    "                       args=training_args,\n",
    "                       train_dataset=train_dataset,\n",
    "                       eval_dataset=test_dataset,\n",
    "                       tokenizer=tokenizer,\n",
    "                       compute_metrics=compute_matrix,\n",
    "#                        callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af59984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0 (NGC 23.03/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
