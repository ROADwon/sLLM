{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f1fa6e",
   "metadata": {},
   "source": [
    "## Try finetuning sLLM step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf85e5",
   "metadata": {},
   "source": [
    "## Notice! \n",
    "-> if you have some problem with datasets library or transformers library\n",
    "---\n",
    "Problem 1. error \"no module named dataset\" <br>\n",
    "solution 1. !pip3 install datasets<br>\n",
    "---\n",
    "Problem 2. huggingface_hub Error <br>\n",
    "Solution 2. ! pip3 install -U transformers <br>\n",
    "\n",
    "--- \n",
    "are those errors belong GPU session was closed,\n",
    "all the installation information was formatting \n",
    "so, if you restart GPU session, you must reinstall all the library, when install library or file before closed session\n",
    "not Kernel restart only GPU session restart\n",
    "---\n",
    "and also follow this solution\n",
    "1. pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63278e7e",
   "metadata": {},
   "source": [
    "### Step0. import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1cd8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.0' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9640a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc43712",
   "metadata": {},
   "source": [
    "### Step1. CUDA define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09a005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\")\n",
    "else:\n",
    "    device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84343361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a57ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c725ea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 25 05:34:43 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  CUDA GPU                       On  | 00000000:E3:00.0 Off |                    0 |\n",
      "| N/A   46C    P0              54W / 300W |     48MiB / 81074MiB |     N/A      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43961b",
   "metadata": {},
   "source": [
    "### Step2. Data and Model, Tokenizer Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23bf58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"flytech/python-codes-25k\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c35a1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this access token is huggingface's API access KEY if you don't have API key then you must make a KEY first\n",
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ef713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pretrained model load from huggingface model hub\n",
    "model_name=\"google/gemma-2b-it\"\n",
    "tokenizer_name=\"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c7cd4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b02a478c64f4d86b1fbdcb8b62434a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            token=access_token,\n",
    "                                            device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fefe7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## this cell for testing small_dataset training\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
    "#                                          token=access_token,\n",
    "#                                          truncation=True,\n",
    "#                                          padding=True,\n",
    "#                                          max_length=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f033317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell for training original dataset training\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
    "                                          token = access_token,\n",
    "                                          truncation=True,\n",
    "                                          padding=True,\n",
    "                                          max_length=200\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50741edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side=\"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c9e3e",
   "metadata": {},
   "source": [
    "### Step3. Dataset insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae1ef6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this DataFrame is not use in this notebooks,\n",
    "## just for show dataset\n",
    "df = pd.DataFrame(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283b2c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>```python\\ntasks = []\\nwhile True:\\n    task =...</td>\n",
       "      <td>Help me set up my daily to-do list!</td>\n",
       "      <td>Setting up your daily to-do list...</td>\n",
       "      <td>Help me set up my daily to-do list! Setting up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>```python\\nshopping_list = {}\\nwhile True:\\n  ...</td>\n",
       "      <td>Create a shopping list based on my inputs!</td>\n",
       "      <td>Creating a shopping list...</td>\n",
       "      <td>Create a shopping list based on my inputs! Cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>```python\\ntotal_time = 0\\nfor i in range(1, 8...</td>\n",
       "      <td>Calculate how much time I spend on my phone pe...</td>\n",
       "      <td>Calculating weekly phone usage...</td>\n",
       "      <td>Calculate how much time I spend on my phone pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              output  \\\n",
       "0  ```python\\ntasks = []\\nwhile True:\\n    task =...   \n",
       "1  ```python\\nshopping_list = {}\\nwhile True:\\n  ...   \n",
       "2  ```python\\ntotal_time = 0\\nfor i in range(1, 8...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0                Help me set up my daily to-do list!   \n",
       "1         Create a shopping list based on my inputs!   \n",
       "2  Calculate how much time I spend on my phone pe...   \n",
       "\n",
       "                                 input  \\\n",
       "0  Setting up your daily to-do list...   \n",
       "1          Creating a shopping list...   \n",
       "2    Calculating weekly phone usage...   \n",
       "\n",
       "                                                text  \n",
       "0  Help me set up my daily to-do list! Setting up...  \n",
       "1  Create a shopping list based on my inputs! Cre...  \n",
       "2  Calculate how much time I spend on my phone pe...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bae474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds =ds.remove_columns([\"input\",\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20360f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'instruction', 'input', 'text'],\n",
       "    num_rows: 49626\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1b61331",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for NatureLanguage To Tokenized Language\n",
    "def tokenize_function(row):\n",
    "    return tokenizer(row[\"instruction\"],row[\"output\"],row[\"input\"],row[\"text\"],padding=True,truncation=True, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78098dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = ds.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c28e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23995117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>```python\\ntasks = []\\nwhile True:\\n    task =...</td>\n",
       "      <td>Help me set up my daily to-do list!</td>\n",
       "      <td>Setting up your daily to-do list...</td>\n",
       "      <td>Help me set up my daily to-do list! Setting up...</td>\n",
       "      <td>[2, 14795, 682, 1142, 908, 970, 7074, 577, 235...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 18101, 908, 861, 7074, 577, 235290, 1042, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>```python\\nshopping_list = {}\\nwhile True:\\n  ...</td>\n",
       "      <td>Create a shopping list based on my inputs!</td>\n",
       "      <td>Creating a shopping list...</td>\n",
       "      <td>Create a shopping list based on my inputs! Cre...</td>\n",
       "      <td>[2, 4912, 476, 9723, 1889, 3482, 611, 970, 193...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 35998, 476, 9723, 1889, 955, 2, 4912, 476,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>```python\\ntotal_time = 0\\nfor i in range(1, 8...</td>\n",
       "      <td>Calculate how much time I spend on my phone pe...</td>\n",
       "      <td>Calculating weekly phone usage...</td>\n",
       "      <td>Calculate how much time I spend on my phone pe...</td>\n",
       "      <td>[2, 51654, 1368, 1683, 1069, 590, 9999, 611, 9...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 144300, 18738, 5248, 14335, 955, 2, 51654,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              output  \\\n",
       "0  ```python\\ntasks = []\\nwhile True:\\n    task =...   \n",
       "1  ```python\\nshopping_list = {}\\nwhile True:\\n  ...   \n",
       "2  ```python\\ntotal_time = 0\\nfor i in range(1, 8...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0                Help me set up my daily to-do list!   \n",
       "1         Create a shopping list based on my inputs!   \n",
       "2  Calculate how much time I spend on my phone pe...   \n",
       "\n",
       "                                 input  \\\n",
       "0  Setting up your daily to-do list...   \n",
       "1          Creating a shopping list...   \n",
       "2    Calculating weekly phone usage...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Help me set up my daily to-do list! Setting up...   \n",
       "1  Create a shopping list based on my inputs! Cre...   \n",
       "2  Calculate how much time I spend on my phone pe...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [2, 14795, 682, 1142, 908, 970, 7074, 577, 235...   \n",
       "1  [2, 4912, 476, 9723, 1889, 3482, 611, 970, 193...   \n",
       "2  [2, 51654, 1368, 1683, 1069, 590, 9999, 611, 9...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [2, 18101, 908, 861, 7074, 577, 235290, 1042, ...  \n",
       "1  [2, 35998, 476, 9723, 1889, 955, 2, 4912, 476,...  \n",
       "2  [2, 144300, 18738, 5248, 14335, 955, 2, 51654,...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "240bfc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenized dataset split\n",
    "tokenized_dataset = tokenized_datasets.train_test_split(test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4eed7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['output', 'instruction', 'input', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 34738\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['output', 'instruction', 'input', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14888\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e82fdeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## this cell for testing small_dataset\n",
    "# small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=41).select(range(1000))\n",
    "# small_test_dataset = tokenized_dataset[\"test\"].shuffle(seed=41).select(range(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7866e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_dataset[\"train\"].shuffle().select(range(1000))\n",
    "test_dataset = tokenized_dataset[\"test\"].shuffle().select(range(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f881178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # small_dataset \n",
    "# small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f24399dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # small_dataset\n",
    "# small_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c48ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'instruction', 'input', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af06d6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'instruction', 'input', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 300\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbc486",
   "metadata": {},
   "source": [
    "### Step4. Config Method and Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d130faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5bdab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## for evaluation metrics\n",
    "# def compute_metrics(eval_pred):\n",
    "#     #logits, labels = eval_pred\n",
    "#     predictions = np.argmax(eval_pred.predictions, axis=-1)\n",
    "#     labels = eval_pred.label_ids\n",
    "    \n",
    "#     eval_dict = {'predictions':predictions.to(),'references':labels.tolist()}\n",
    "#     return metric.compute(**eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2a787b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from evaluation metics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b48f70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acc5c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this cell for testing small-dataset\n",
    "# training_args = TrainingArguments(output_dir=\"./finetune_results\",\n",
    "#                                   num_train_epochs=4,\n",
    "#                                  per_device_train_batch_size=1,\n",
    "#                                  per_device_eval_batch_size=1,\n",
    "#                                  gradient_accumulation_steps=4,\n",
    "#                                 logging_steps=10,\n",
    "#                                   learning_rate=2e-3,\n",
    "#                                   eval_accumulation_steps=8\n",
    "#                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c292831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./finetune_results_2\",\n",
    "                                 num_train_epochs=3,\n",
    "                                 per_device_train_batch_size=2,\n",
    "                                 per_device_eval_batch_size=1, \n",
    "                                 gradient_accumulation_steps=4,\n",
    "                                  eval_accumulation_steps=10,\n",
    "                                 logging_steps=10,\n",
    "                                 learning_rate=2e-3, \n",
    "                                 warmup_steps=10,\n",
    "                                 weight_decay=0.03,\n",
    "                                 logging_dir=\"./finetune_logs_2\",\n",
    "                                 do_eval=True,\n",
    "                                  eval_steps=100,\n",
    "                                  save_steps=100,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                 load_best_model_at_end=True,\n",
    "                                 dataloader_num_workers=multiprocessing.cpu_count(),\n",
    "                                 optim=\"adamw_torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "732bfa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this cell for testing small dataset\n",
    "# trainer = Trainer(model=model,\n",
    "#                  args=training_args,\n",
    "#                  train_dataset=small_train_dataset,\n",
    "#                  eval_dataset=small_test_dataset,\n",
    "#                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c982c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                 args=training_args,\n",
    "                 train_dataset=train_dataset,\n",
    "                 eval_dataset=test_dataset,\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa02e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrlfdnjs9839\u001b[0m (\u001b[33mlineworld\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/sLLM/wandb/run-20240425_053513-3f03xetu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/lineworld/huggingface/runs/3f03xetu\" target=\"_blank\">misunderstood-wind-91</a></strong> to <a href=\"https://wandb.ai/lineworld/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/375 01:18 < 02:37, 1.58 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 02:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [[     2    476    476 ... 235248 235248 235248]\n [     2    476    476 ...      1      1      1]\n [     2    476    476 ...      1      1    108]\n ...\n [     2 233414    476 ...      1      1      1]\n [     2    476    476 ...      1      1      1]\n [     2 233414    476 ...      1      1      1]],\nInput references: [[     2      2   1841 ... 235265    109    651]\n [     2      2   4912 ...      1      1      1]\n [     2      2  27964 ... 235248 235284 235265]\n ...\n [     2      2 121784 ...      1      1      1]\n [     2      2  27964 ...      1      1      1]\n [     2      2  38557 ...      1      1      1]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2298\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2302\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2660\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2662\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3464\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3466\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3467\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3477\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:3719\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3715\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3716\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3717\u001b[0m         )\n\u001b[1;32m   3718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3719\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m      3\u001b[0m logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/evaluate/module.py:450\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/evaluate/module.py:541\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [[     2    476    476 ... 235248 235248 235248]\n [     2    476    476 ...      1      1      1]\n [     2    476    476 ...      1      1    108]\n ...\n [     2 233414    476 ...      1      1      1]\n [     2    476    476 ...      1      1      1]\n [     2 233414    476 ...      1      1      1]],\nInput references: [[     2      2   1841 ... 235265    109    651]\n [     2      2   4912 ...      1      1      1]\n [     2      2  27964 ... 235248 235284 235265]\n ...\n [     2      2 121784 ...      1      1      1]\n [     2      2  27964 ...      1      1      1]\n [     2      2  38557 ...      1      1      1]]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e75894",
   "metadata": {},
   "source": [
    "### Step5. Save a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name=\"line_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a16acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b12d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(new_model_name, use_temp_dir=False)\n",
    "# tokenizer.push_to_hub(new_model_name, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a1e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0 (NGC 23.03/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
